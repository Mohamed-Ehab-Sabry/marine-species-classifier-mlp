{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fdafdd8",
   "metadata": {},
   "source": [
    "# Assignemnt 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa25e38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f68b336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Class 0: Clams\n",
      "Loading Class 1: Corals\n",
      "Loading Class 2: Crabs\n",
      "Loading Class 3: Dolphin\n",
      "Loading Class 4: Eel\n",
      "Loading Class 5: Fish\n",
      "Loading Class 6: Jelly_Fish\n",
      "Loading Class 7: Lobster\n",
      "Loading Class 8: Nudibranchs\n",
      "Loading Class 9: Octopus\n",
      "Loading Class 10: Otter\n",
      "Loading Class 11: Penguin\n",
      "Loading Class 12: Puffers\n",
      "Loading Class 13: Sea_Rays\n",
      "Loading Class 14: Sea_Urchins\n",
      "Loading Class 15: Seahorse\n",
      "Loading Class 16: Seal\n",
      "Loading Class 17: Sharks\n",
      "Loading Class 18: Shrimp\n",
      "Loading Class 19: Squid\n",
      "Loading Class 20: Starfish\n",
      "Loading Class 21: Turtle_Tortoise\n",
      "Loading Class 22: Whale\n",
      "[[0.34117647 0.36078431 0.23137255 ... 0.54901961 0.74901961 0.63529412]\n",
      " [0.41568627 0.41176471 0.43529412 ... 0.41568627 0.38823529 0.26666667]\n",
      " [0.36078431 0.38431373 0.25490196 ... 0.92156863 0.89019608 0.64705882]\n",
      " ...\n",
      " [0.74901961 0.50980392 0.37647059 ... 0.7372549  0.4627451  0.31764706]\n",
      " [0.6745098  0.50196078 0.36470588 ... 0.25882353 0.23529412 0.14509804]\n",
      " [0.93333333 0.78039216 0.65490196 ... 0.85490196 0.70980392 0.61568627]]\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def one_hot_encoding(img_true_classes, num_of_classes):\n",
    "    num_of_images = img_true_classes.shape[0]\n",
    "    one_hot_encoded_matrix = np.zeros((num_of_images, num_of_classes))\n",
    "    one_hot_encoded_matrix[np.arange(num_of_images), img_true_classes] = 1\n",
    "\n",
    "    return one_hot_encoded_matrix\n",
    "\n",
    "def load_dataset(data_dir, img_size = 64):\n",
    "    features = []\n",
    "    labels = []\n",
    "\n",
    "    classes_names = [entry.name for entry in os.scandir(data_dir) if entry.is_dir()]\n",
    "    classes_names = sorted(classes_names)\n",
    "\n",
    "    for class_index, class_name in enumerate(classes_names):\n",
    "        class_path = os.path.join(data_dir,class_name)\n",
    "        print(f\"Loading Class {class_index}: {class_name}\")\n",
    "\n",
    "        for img_name in os.listdir(class_path):\n",
    "            img_path = os.path.join(class_path, img_name)\n",
    "            \n",
    "            try:\n",
    "                img_np_arr = cv2.imread(img_path)\n",
    "\n",
    "                if img_np_arr is None:\n",
    "                    continue\n",
    "                \n",
    "                img_resized_arr = cv2.resize(img_np_arr, (img_size,img_size))\n",
    "                img_flatten_arr = img_resized_arr.flatten()\n",
    "                features.append(img_flatten_arr)\n",
    "                labels.append(class_index)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {img_name}: {e}\")\n",
    "\n",
    "\n",
    "    X = np.array(features)\n",
    "    y = np.array(labels)\n",
    "\n",
    "    X = X/255\n",
    "    y = one_hot_encoding(y, len(classes_names))\n",
    "\n",
    "    return X, y, classes_names\n",
    "\n",
    "\n",
    "\n",
    "X, y, classes_names = load_dataset(r\"C:\\\\Life\\\\FCAI_Stuff\\\\Third_year_AI\\\\Intro_to_ML\\Assignments\\Sea_Animals\")\n",
    "print(X)\n",
    "print(y)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81516849",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer_size = [X.shape[1]] # Number of features per image\n",
    "output_layer_size = [len(classes_names)]\n",
    "\n",
    "def build_nueral_network(in_size, out_size):\n",
    "    hid_layers_num = int(input(\"So, how many hidden layers do you need for your MLP ?\\nNumber of hidden layers: \"))\n",
    "    hid_sizes = []\n",
    "\n",
    "    for i in range(hid_layers_num):\n",
    "        hid_layer_size = int(input(f\"Please enter the number of nuerons that you need in layer number {i + 1}: \"))\n",
    "        hid_sizes.append(hid_layer_size)\n",
    "\n",
    "    activ_func = int(input(\"\"\"\n",
    "                           Lastly, what is the Activation function you want to use in the hidden layers?\n",
    "                           1) Sigmoid\n",
    "                           2) Relu\n",
    "                           Choose (1/2): \n",
    "                           \"\"\"))\n",
    "\n",
    "    net_sizes = in_size + hid_sizes + out_size\n",
    "    return net_sizes , activ_func\n",
    "\n",
    "net_sizes, activ_func = build_nueral_network(input_layer_size, output_layer_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56725a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Structure: [12288, 100, 90, 80, 70, 60, 50, 40, 30, 20, 10, 23]\n",
      "Second Weight Matrix shape: (100, 12288)\n",
      "Second Bias Vector shape: (100, 1)\n"
     ]
    }
   ],
   "source": [
    "def intialize_param(net_sizes):        \n",
    "    weights = []\n",
    "    biases = []\n",
    "\n",
    "    for i in range(len(net_sizes) - 1):\n",
    "        input_dim = net_sizes[i] \n",
    "        output_dim =  net_sizes[i + 1]\n",
    "        weight = np.random.randn(output_dim, input_dim) * 0.01\n",
    "        weights.append(weight)\n",
    "        bias = np.zeros((output_dim, 1))\n",
    "        biases.append(bias)\n",
    "\n",
    "    return weights, biases\n",
    "\n",
    "weights, biases = intialize_param(net_sizes)\n",
    "print(f\"Structure: {net_sizes}\")\n",
    "print(f\"first Weight Matrix shape: {weights[0].shape}\")\n",
    "print(f\"first Bias Vector shape: {biases[0].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d959610",
   "metadata": {},
   "source": [
    "# Activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07fbd3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def Relu(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def SoftMax(z):\n",
    "    # 1. Numerical Stability Trick\n",
    "    # If Z contains large numbers wil cause an overflow error.\n",
    "    # By subtracting the max value from every column, the largest number becomes 0.\n",
    "    # e^0 = 1, which is safe. The math result remains identical.\n",
    "    # axis=0 means \"find max down the column\" (for each image separately).\n",
    "    # keepdims = true => keeps the values that we divide the matrix by in a proper shape to do so (as in vecotr not elements)\n",
    "\n",
    "    # SoftMax = e^z/sum(e^z)\n",
    "    shift_z = z - np.max(z, axis=0, keepdims=True)\n",
    "    exp_z = np.exp(shift_z)\n",
    "    sum_exp_z = np.sum(exp_z, axis=0, keepdims= True)\n",
    "\n",
    "    A = exp_z / sum_exp_z\n",
    "\n",
    "    return A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790a3b33",
   "metadata": {},
   "source": [
    "# Forward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba0cae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, weights, biases, activ_func):\n",
    "\n",
    "    A = X.T # input for each layer initialized with the input layer\n",
    "\n",
    "    # to save the values of A(inputs to the layer) and Z(Netj) for the backward propagation\n",
    "    # Why save Z? simply because we use its derivative in the backword path\n",
    "    cache_A = []\n",
    "    cache_Z = []\n",
    "    cache_A.append(A)\n",
    "\n",
    "    # enumerate gives automatic counter inside loop without the need to manually increment\n",
    "    # it returns [index, tubel(weights, biases)]\n",
    "    for layer_index, (w,b) in enumerate(zip(weights, biases)):\n",
    "        Z = np.dot(w,A) + b # Netj = Z = wx + b\n",
    "        cache_Z.append(Z)\n",
    "\n",
    "        if layer_index == len(weights) - 1: # if it is the ouput layer\n",
    "            A = SoftMax(Z)\n",
    "            cache_A.append(A)\n",
    "        else:\n",
    "            if activ_func == 1: # hidden with Sigmoid as an activation function\n",
    "                A = Sigmoid(Z)\n",
    "                cache_A.append(A)\n",
    "            elif activ_func == 2: # hidden with Relu as an acitivation function\n",
    "                A = Relu(Z)\n",
    "                cache_A.append(A)\n",
    "    \n",
    "    return cache_A, cache_Z\n",
    "\n",
    "a_cache, z_cache = forward_propagation(X,weights,biases,activ_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d5fa25",
   "metadata": {},
   "source": [
    "# Backward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4865784",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_derivative(z):\n",
    "    sig = Sigmoid(z)\n",
    "    return sig * (1 - sig)\n",
    "\n",
    "def relu_derivative(z):\n",
    "    return (z > 0).astype(float)\n",
    "\n",
    "def backward_propagation(cache_A, cache_Z, weights, y, activ_func, m)\n",
    "    num_layers = len(weights)\n",
    "    weight_gradients = []\n",
    "    bias_gradients = []\n",
    "\n",
    "    # dA = Predictions - True Labels\n",
    "    dA = cache_A[-1] - y.T\n",
    "\n",
    "    # Backpropagate through each layer (from last to first)\n",
    "    for layer_index in range(num_layers - 1, -1, -1):\n",
    "        # Get Z and A for this layer\n",
    "        Z_curr = cache_Z[layer_index]\n",
    "        A_prev = cache_A[layer_index]\n",
    "        W_curr = weights[layer_index]\n",
    "\n",
    "        # Calculate dZ (derivative with respect to Z)\n",
    "        if layer_index == num_layers - 1:\n",
    "            dZ = dA\n",
    "        else:\n",
    "            # Hidden layers: dZ = dA * activation_derivative(Z)\n",
    "            if activ_func == 1:  # Sigmoid\n",
    "                dZ = dA * sigmoid_derivative(Z_curr)\n",
    "            elif activ_func == 2:  # ReLU\n",
    "                dZ = dA * relu_derivative(Z_curr)\n",
    "        \n",
    "        # Calculate dW (gradient for weights)\n",
    "        # dW = (1/m) * dZ * A_prev.T\n",
    "        dW = (1/m) * np.dot(dZ, A_prev.T)\n",
    "        weight_gradients.insert(0, dW)\n",
    "        \n",
    "        # Calculate db (gradient for biases)\n",
    "        # db = (1/m) * sum of dZ along all samples\n",
    "        db = (1/m) * np.sum(dZ, axis=1, keepdims=True)\n",
    "        bias_gradients.insert(0, db)\n",
    "        \n",
    "        # Calculate dA for next iteration (previous layer)\n",
    "        # dA_prev = W.T * dZ\n",
    "        if layer_index > 0:\n",
    "            dA = np.dot(W_curr.T, dZ)\n",
    "    \n",
    "    return weight_gradients, bias_gradients\n",
    "\n",
    "# Test the function\n",
    "weight_grads, bias_grads = backward_propagation(a_cache, z_cache, weights, y, activ_func, X.shape[0])\n",
    "print(f\"Number of weight gradients: {len(weight_grads)}\")\n",
    "print(f\"First weight gradient shape: {weight_grads[0].shape}\")\n",
    "print(f\"First bias gradient shape: {bias_grads[0].shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c771bc62",
   "metadata": {},
   "source": [
    "# Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17c58d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(predictions, y_true, m):\n",
    "    # Add small epsilon to avoid log(0)\n",
    "    epsilon = 1e-15\n",
    "    predictions = np.clip(predictions, epsilon, 1 - epsilon)\n",
    "    \n",
    "    # Cross-entropy for multi-class classification\n",
    "    # y_true.T is (num_classes, m), predictions is (num_classes, m)\n",
    "    # We multiply element-wise and sum\n",
    "    loss = -(1/m) * np.sum(y_true.T * np.log(predictions))\n",
    "    \n",
    "    return loss\n",
    "\n",
    "# Test the loss function\n",
    "test_loss = calculate_loss(a_cache[-1], y, X.shape[0])\n",
    "print(f\"Initial Loss: {test_loss}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bb5e94",
   "metadata": {},
   "source": [
    "# Update Parameters (Gradient Descent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcd7de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(weights, biases, weight_gradients, bias_gradients, learning_rate):\n",
    "    updated_weights = []\n",
    "    updated_biases = []\n",
    "    \n",
    "    for i in range(len(weights)):\n",
    "        # Update weights: W = W - learning_rate * dW\n",
    "        updated_W = weights[i] - learning_rate * weight_gradients[i]\n",
    "        updated_weights.append(updated_W)\n",
    "        \n",
    "        # Update biases: b = b - learning_rate * db\n",
    "        updated_b = biases[i] - learning_rate * bias_gradients[i]\n",
    "        updated_biases.append(updated_b)\n",
    "    \n",
    "    return updated_weights, updated_biases\n",
    "\n",
    "# Test the update function\n",
    "learning_rate = 0.01\n",
    "updated_weights, updated_biases = update_parameters(weights, biases, weight_grads, bias_grads, learning_rate)\n",
    "print(f\"Weights updated successfully!\")\n",
    "print(f\"First weight before update: {weights[0][0, 0]:.6f}\")\n",
    "print(f\"First weight after update: {updated_weights[0][0, 0]:.6f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
