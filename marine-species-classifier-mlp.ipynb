{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fdafdd8",
   "metadata": {},
   "source": [
    "# Assignemnt 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6de2465",
   "metadata": {},
   "source": [
    "# Loading the dataset, normalizing the imgaes pixel values, and one-hot encoding the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6f68b336",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 # Library for dealing with images, the biggest and the fastest, written in cpp, wrapped in python.\n",
    "import os # Library for dealing wiht the operating system, getting folders, files, and much much more.\n",
    "import numpy as org_np # Numerical Python, library for doing most of the mathematical operations needed in machine learning.\n",
    "import cupy as np # NumPy, but runs on the GPU.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "def one_hot_encoding(img_true_classes, num_of_classes):\n",
    "    num_of_images = img_true_classes.shape[0]\n",
    "    one_hot_encoded_matrix = np.zeros((num_of_images, num_of_classes))\n",
    "    one_hot_encoded_matrix[np.arange(num_of_images), img_true_classes] = 1\n",
    "\n",
    "    return one_hot_encoded_matrix\n",
    "\n",
    "def load_dataset(data_dir, img_size = 64):\n",
    "    features = []\n",
    "    labels = []\n",
    "\n",
    "    classes_names = [entry.name for entry in os.scandir(data_dir) if entry.is_dir()]\n",
    "    classes_names = sorted(classes_names)\n",
    "\n",
    "    for class_index, class_name in enumerate(classes_names):\n",
    "        class_path = os.path.join(data_dir,class_name)\n",
    "        print(f\"Loading Class {class_index}: {class_name}\")\n",
    "\n",
    "        for img_name in os.listdir(class_path):\n",
    "            img_path = os.path.join(class_path, img_name)\n",
    "            \n",
    "            try:\n",
    "                img_np_arr = cv2.imread(img_path)\n",
    "\n",
    "                if img_np_arr is None:\n",
    "                    continue\n",
    "                \n",
    "                img_resized_arr = cv2.resize(img_np_arr, (img_size,img_size))\n",
    "                img_flatten_arr = img_resized_arr.flatten()\n",
    "                features.append(img_flatten_arr)\n",
    "                labels.append(class_index)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {img_name}: {e}\")\n",
    "\n",
    "\n",
    "    X = np.array(features)\n",
    "    y = np.array(labels)\n",
    "\n",
    "    X = X/255\n",
    "    y = one_hot_encoding(y, len(classes_names))\n",
    "\n",
    "    return X, y, classes_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8128a7f2",
   "metadata": {},
   "source": [
    "# Data Spliting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3fef805a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(X, y, train_ratio = 0.7, v_ratio = 0.15, test_ratio = 0.15):\n",
    "    num_of_imgs = X.shape[0]\n",
    "    indices = np.random.permutation(num_of_imgs)\n",
    "    train_index = int(train_ratio * num_of_imgs)\n",
    "    v_index = int(v_ratio * num_of_imgs)\n",
    "    test_index = int(test_ratio * num_of_imgs) # just for indication, not really needed\n",
    "\n",
    "    train_idx = indices[:train_index]\n",
    "    v_idx = indices[train_index: train_index + v_index]\n",
    "    test_idx = indices[train_index + v_index:]\n",
    "\n",
    "    X_train, X_validation, X_test = X[train_idx], X[v_idx], X[test_idx]\n",
    "    y_train, y_validation, y_test = y[train_idx], y[v_idx], y[test_idx]\n",
    "\n",
    "    return X_train, X_validation, X_test, y_train, y_validation, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569fda01",
   "metadata": {},
   "source": [
    "# Choosing the number of hidden layers, and the activation function for the hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "81516849",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_nueral_network(in_size, out_size):\n",
    "    hid_layers_num = int(input(\"So, how many hidden layers do you need for your MLP ?\\nNumber of hidden layers: \"))\n",
    "    hid_sizes = []\n",
    "\n",
    "    for i in range(hid_layers_num):\n",
    "        hid_layer_size = int(input(f\"Please enter the number of nuerons that you need in layer number {i + 1}: \"))\n",
    "        hid_sizes.append(hid_layer_size)\n",
    "\n",
    "    activ_func = int(input(\"\"\"\n",
    "                           Lastly, what is the Activation function you want to use in the hidden layers?\n",
    "                           1) Sigmoid\n",
    "                           2) Relu\n",
    "                           Choose (1/2): \n",
    "                           \"\"\"))\n",
    "\n",
    "    net_sizes = in_size + hid_sizes + out_size\n",
    "    return net_sizes , activ_func"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414b6903",
   "metadata": {},
   "source": [
    "# Intializing the first weights and biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "56725a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intialize_param(net_sizes):        \n",
    "    weights = []\n",
    "    biases = []\n",
    "\n",
    "    for i in range(len(net_sizes) - 1):\n",
    "        input_dim = net_sizes[i] \n",
    "        output_dim =  net_sizes[i + 1]\n",
    "        weight = np.random.randn(output_dim, input_dim) * 0.01\n",
    "        weights.append(weight)\n",
    "        bias = np.zeros((output_dim, 1))\n",
    "        biases.append(bias)\n",
    "\n",
    "    return weights, biases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d959610",
   "metadata": {},
   "source": [
    "# Activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "07fbd3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def Relu(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def SoftMax(z):\n",
    "    # 1. Numerical Stability Trick\n",
    "    # If Z contains large numbers wil cause an overflow error.\n",
    "    # By subtracting the max value from every column, the largest number becomes 0.\n",
    "    # e^0 = 1, which is safe. The math result remains identical.\n",
    "    # axis=0 means \"find max down the column\" (for each image separately).\n",
    "    # keepdims = true => keeps the values that we divide the matrix by in a proper shape to do so (as in vecotr not elements)\n",
    "\n",
    "    # SoftMax = e^z/sum(e^z)\n",
    "    shift_z = z - np.max(z, axis=0, keepdims=True)\n",
    "    exp_z = np.exp(shift_z)\n",
    "    sum_exp_z = np.sum(exp_z, axis=0, keepdims= True)\n",
    "\n",
    "    A = exp_z / sum_exp_z\n",
    "\n",
    "    return A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790a3b33",
   "metadata": {},
   "source": [
    "# Forward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4ba0cae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, weights, biases, activ_func):\n",
    "\n",
    "    A = X.T # input for each layer initialized with the input layer\n",
    "\n",
    "    # to save the values of A(inputs to the layer) and Z(Netj) for the backward propagation\n",
    "    # Why save Z? simply because we use its derivative in the backword path\n",
    "    cache_A = []\n",
    "    cache_Z = []\n",
    "    cache_A.append(A)\n",
    "\n",
    "    # enumerate gives automatic counter inside loop without the need to manually increment\n",
    "    # it returns [index, tubel(weights, biases)]\n",
    "    for layer_index, (w,b) in enumerate(zip(weights, biases)):\n",
    "        Z = np.dot(w,A) + b # Netj = Z = wx + b\n",
    "        cache_Z.append(Z)\n",
    "\n",
    "        if layer_index == len(weights) - 1: # if it is the ouput layer\n",
    "            A = SoftMax(Z)\n",
    "            cache_A.append(A)\n",
    "        else:\n",
    "            if activ_func == 1: # hidden with Sigmoid as an activation function\n",
    "                A = Sigmoid(Z)\n",
    "                cache_A.append(A)\n",
    "            elif activ_func == 2: # hidden with Relu as an acitivation function\n",
    "                A = Relu(Z)\n",
    "                cache_A.append(A)\n",
    "    \n",
    "    return cache_A, cache_Z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d5fa25",
   "metadata": {},
   "source": [
    "# Backward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b4865784",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_derivative(z):\n",
    "    sig = Sigmoid(z)\n",
    "    return sig * (1 - sig)\n",
    "\n",
    "def relu_derivative(z):\n",
    "    return (z > 0).astype(float)\n",
    "\n",
    "def backward_propagation(cache_A, cache_Z, weights, y, activ_func, m):\n",
    "    num_layers = len(weights)\n",
    "    weight_gradients = []\n",
    "    bias_gradients = []\n",
    "\n",
    "    # dA = Predictions - True Labels\n",
    "    dA = cache_A[-1] - y.T\n",
    "\n",
    "    # Backpropagate through each layer (from last to first)\n",
    "    for layer_index in range(num_layers - 1, -1, -1):\n",
    "        # Get Z and A for this layer\n",
    "        Z_curr = cache_Z[layer_index]\n",
    "        A_prev = cache_A[layer_index]\n",
    "        W_curr = weights[layer_index]\n",
    "\n",
    "        # Calculate dZ (derivative with respect to Z)\n",
    "        if layer_index == num_layers - 1:\n",
    "            dZ = dA\n",
    "        else:\n",
    "            # Hidden layers: dZ = dA * activation_derivative(Z)\n",
    "            if activ_func == 1:  # Sigmoid\n",
    "                dZ = dA * sigmoid_derivative(Z_curr)\n",
    "            elif activ_func == 2:  # ReLU\n",
    "                dZ = dA * relu_derivative(Z_curr)\n",
    "        \n",
    "        # Calculate dW (gradient for weights)\n",
    "        # dW = (1/m) * dZ * A_prev.T\n",
    "        dW = (1/m) * np.dot(dZ, A_prev.T)\n",
    "        weight_gradients.insert(0, dW)\n",
    "        \n",
    "        # Calculate db (gradient for biases)\n",
    "        # db = (1/m) * sum of dZ along all samples\n",
    "        db = (1/m) * np.sum(dZ, axis=1, keepdims=True)\n",
    "        bias_gradients.insert(0, db)\n",
    "        \n",
    "        # Calculate dA for next iteration (previous layer)\n",
    "        # dA_prev = W.T * dZ\n",
    "        if layer_index > 0:\n",
    "            dA = np.dot(W_curr.T, dZ)\n",
    "    \n",
    "    return weight_gradients, bias_gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c771bc62",
   "metadata": {},
   "source": [
    "# Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f17c58d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(predictions, y_true, m):\n",
    "    # Add small epsilon to avoid log(0)\n",
    "    epsilon = 1e-15\n",
    "    predictions = np.clip(predictions, epsilon, 1 - epsilon)\n",
    "    \n",
    "    # Cross-entropy for multi-class classification\n",
    "    # y_true.T is (num_classes, m), predictions is (num_classes, m)\n",
    "    # We multiply element-wise and sum\n",
    "    loss = -(1/m) * np.sum(y_true.T * np.log(predictions))\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bb5e94",
   "metadata": {},
   "source": [
    "# Update Parameters (Gradient Descent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9fcd7de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(weights, biases, weight_gradients, bias_gradients, learning_rate):\n",
    "    updated_weights = []\n",
    "    updated_biases = []\n",
    "    \n",
    "    for i in range(len(weights)):\n",
    "        # Update weights: W = W - learning_rate * dW\n",
    "        updated_W = weights[i] - learning_rate * weight_gradients[i]\n",
    "        updated_weights.append(updated_W)\n",
    "        \n",
    "        # Update biases: b = b - learning_rate * db\n",
    "        updated_b = biases[i] - learning_rate * bias_gradients[i]\n",
    "        updated_biases.append(updated_b)\n",
    "    \n",
    "    return updated_weights, updated_biases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798b4460",
   "metadata": {},
   "source": [
    "# Actual Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8d23ea08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_acc(X, y_true_one_hot, weights, biases, activ_func):\n",
    "    a_cache, _ = forward_propagation(X, weights, biases, activ_func)\n",
    "    probabilities = a_cache[-1]\n",
    "\n",
    "    predictions = np.argmax(probabilities, axis = 0)\n",
    "    true_labels = np.argmax(y_true_one_hot, axis = 1)\n",
    "\n",
    "    acc = np.mean(predictions == true_labels)\n",
    "    return acc * 100\n",
    "\n",
    "\n",
    "def train(X_train, y_train, X_test, y_test, net_sizes, activ_func, epochs = 1000, learning_rate = 0.01):\n",
    "\n",
    "    num_of_training_imgs = X_train.shape[0]\n",
    "    weights, biases = intialize_param(net_sizes)\n",
    "\n",
    "    loss_history = []\n",
    "    train_acc_history = []\n",
    "    test_acc_history = []\n",
    "\n",
    "    print(f\"Training on GPU for {epochs} epochs...\")\n",
    "\n",
    "    for i in range(epochs):\n",
    "        a_cache, z_cache = forward_propagation(X_train,weights,biases,activ_func)\n",
    "        weight_grads, bias_grads = backward_propagation(a_cache, z_cache, weights, y_train, activ_func, num_of_training_imgs)       \n",
    "        updated_weights, updated_biases = update_parameters(weights, biases, weight_grads, bias_grads, learning_rate)\n",
    "        weights, biases = updated_weights, updated_biases\n",
    "\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            current_loss = calculate_loss(a_cache[-1], y_train, num_of_training_imgs)\n",
    "            train_acc = calc_acc(X_train, y_train, weights, biases, activ_func)\n",
    "            test_acc = calc_acc(X_test, y_test, weights, biases, activ_func)\n",
    "\n",
    "            loss_history.append(current_loss)\n",
    "            train_acc_history.append(train_acc)\n",
    "            test_acc_history.append(test_acc)\n",
    "            print(f\"Epoch {i}: Loss {current_loss: .4f} | Train Acc: {train_acc: .1f}% | Test Acc: {test_acc: .1f}%\")\n",
    "\n",
    "    return weights, biases, loss_history, train_acc_history, test_acc_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0c4030",
   "metadata": {},
   "source": [
    "# Prediction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9d3a0080",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X_test, y_test, weights, biases,activ_func, classes_names):\n",
    "    a_cache, _ = forward_propagation(X_test, weights, biases, activ_func)\n",
    "    probabilities_of_output_class = a_cache[-1]\n",
    "    predicted_indices_gpu = np.argmax(probabilities_of_output_class, axis = 0)\n",
    "    true_indices_gpu = np.argmax(y_test, axis = 1)\n",
    "    predicted_indices_cpu = predicted_indices_gpu.get()\n",
    "    true_indices_cpu = true_indices_gpu.get()\n",
    "\n",
    "    predicted_names = org_np.array(classes_names)[predicted_indices_cpu]\n",
    "    true_names = org_np.argmax(classes_names)[true_indices_cpu]\n",
    "\n",
    "    return predicted_names, true_names\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9104399f",
   "metadata": {},
   "source": [
    "# Function for plotting some of the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "671e520f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(loss_history, train_acc, test_acc):\n",
    "    # Plot Loss and Accuracy side by side\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Subplot 1: Loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(loss_history, label='Training Loss', color='red')\n",
    "    plt.title('Loss over Epochs')\n",
    "    plt.xlabel('Epochs (x100)')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Subplot 2: Accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_acc, label='Train Accuracy', color='blue')\n",
    "    plt.plot(test_acc, label='Test Accuracy', color='green')\n",
    "    plt.title('Accuracy over Epochs')\n",
    "    plt.xlabel('Epochs (x100)')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6c8971",
   "metadata": {},
   "source": [
    "# Running the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0dcfe40a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Class 0: Clams\n",
      "Loading Class 1: Corals\n",
      "Loading Class 2: Crabs\n",
      "Loading Class 3: Dolphin\n",
      "Loading Class 4: Eel\n",
      "Loading Class 5: Fish\n",
      "Loading Class 6: Jelly_Fish\n",
      "Loading Class 7: Lobster\n",
      "Loading Class 8: Nudibranchs\n",
      "Loading Class 9: Octopus\n",
      "Loading Class 10: Otter\n",
      "Loading Class 11: Penguin\n",
      "Loading Class 12: Puffers\n",
      "Loading Class 13: Sea_Rays\n",
      "Loading Class 14: Sea_Urchins\n",
      "Loading Class 15: Seahorse\n",
      "Loading Class 16: Seal\n",
      "Loading Class 17: Sharks\n",
      "Loading Class 18: Shrimp\n",
      "Loading Class 19: Squid\n",
      "Loading Class 20: Starfish\n",
      "Loading Class 21: Turtle_Tortoise\n",
      "Loading Class 22: Whale\n",
      "\n",
      "Starting Training on GPU...\n",
      "Training on GPU for 2000 epochs...\n",
      "Epoch 0: Loss  3.1358 | Train Acc:  13.7% | Test Acc:  15.2%\n",
      "Epoch 100: Loss  2.7454 | Train Acc:  18.8% | Test Acc:  19.8%\n",
      "Epoch 200: Loss  2.6203 | Train Acc:  21.2% | Test Acc:  22.3%\n",
      "Epoch 300: Loss  2.5429 | Train Acc:  23.8% | Test Acc:  24.6%\n",
      "Epoch 400: Loss  2.4652 | Train Acc:  26.4% | Test Acc:  25.5%\n",
      "Epoch 500: Loss  2.4569 | Train Acc:  27.7% | Test Acc:  27.0%\n",
      "Epoch 600: Loss  2.3886 | Train Acc:  28.5% | Test Acc:  26.7%\n",
      "Epoch 700: Loss  2.3633 | Train Acc:  28.8% | Test Acc:  27.9%\n",
      "Epoch 800: Loss  2.3344 | Train Acc:  30.3% | Test Acc:  28.2%\n",
      "Epoch 900: Loss  2.2846 | Train Acc:  31.1% | Test Acc:  28.3%\n",
      "Epoch 1000: Loss  2.2561 | Train Acc:  31.7% | Test Acc:  27.8%\n",
      "Epoch 1100: Loss  2.2135 | Train Acc:  32.5% | Test Acc:  27.4%\n",
      "Epoch 1200: Loss  2.2101 | Train Acc:  33.8% | Test Acc:  30.0%\n",
      "Epoch 1300: Loss  2.1988 | Train Acc:  33.1% | Test Acc:  27.6%\n",
      "Epoch 1400: Loss  2.1637 | Train Acc:  36.2% | Test Acc:  29.9%\n",
      "Epoch 1500: Loss  2.1318 | Train Acc:  33.7% | Test Acc:  29.2%\n",
      "Epoch 1600: Loss  2.0368 | Train Acc:  37.3% | Test Acc:  30.1%\n",
      "Epoch 1700: Loss  2.1296 | Train Acc:  38.2% | Test Acc:  29.6%\n",
      "Epoch 1800: Loss  1.9988 | Train Acc:  39.0% | Test Acc:  28.2%\n",
      "Epoch 1900: Loss  2.1417 | Train Acc:  39.8% | Test Acc:  28.3%\n",
      "\n",
      "Running Prediction on Test Set...\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "invalid index to scalar variable.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     10\u001b[39m trained_weights, trained_biases, loss_hist, train_acc_hist, test_acc_hist = train(X_train, y_train, X_test, y_test, net_sizes, activ_func, epochs = \u001b[32m2000\u001b[39m, learning_rate = \u001b[32m0.1\u001b[39m)\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mRunning Prediction on Test Set...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m pred_names, true_names = \u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrained_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrained_biases\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactiv_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclasses_names\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- Test Results ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     16\u001b[39m correct_count = \u001b[32m0\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 10\u001b[39m, in \u001b[36mpredict\u001b[39m\u001b[34m(X_test, y_test, weights, biases, activ_func, classes_names)\u001b[39m\n\u001b[32m      7\u001b[39m true_indices_cpu = true_indices_gpu.get()\n\u001b[32m      9\u001b[39m predicted_names = org_np.array(classes_names)[predicted_indices_cpu]\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m true_names = \u001b[43morg_np\u001b[49m\u001b[43m.\u001b[49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclasses_names\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrue_indices_cpu\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m predicted_names, true_names\n",
      "\u001b[31mIndexError\u001b[39m: invalid index to scalar variable."
     ]
    }
   ],
   "source": [
    "X, y, classes_names = load_dataset(r\"C:\\\\Life\\\\FCAI_Stuff\\\\Third_year_AI\\\\Intro_to_ML\\Assignments\\Sea_Animals\")\n",
    "X_train, X_v, X_test, y_train, y_v, y_test = split_data(X, y, train_ratio = 0.7, v_ratio = 0.15, test_ratio = 0.15)\n",
    "\n",
    "input_layer_size = [X_train.shape[1]] # 1) Number of features per image\n",
    "output_layer_size = [len(classes_names)] # 2) Number of classes\n",
    "\n",
    "net_sizes, activ_func = build_nueral_network(input_layer_size, output_layer_size)\n",
    "\n",
    "print(\"\\nStarting Training on GPU...\")\n",
    "trained_weights, trained_biases, loss_hist, train_acc_hist, test_acc_hist = train(X_train, y_train, X_test, y_test, net_sizes, activ_func, epochs = 2000, learning_rate = 0.1)\n",
    "\n",
    "print(\"\\nRunning Prediction on Test Set...\")\n",
    "pred_names, true_names = predict(X_test, y_test, trained_weights, trained_biases, activ_func, classes_names)\n",
    "\n",
    "print(\"\\n--- Test Results ---\")\n",
    "correct_count = 0\n",
    "total_count = len(pred_names)\n",
    "\n",
    "for i in range(total_count):\n",
    "    if pred_names[i] == true_names[i]:\n",
    "        correct_count += 1\n",
    "    # Simple print format\n",
    "    print(f\"Predicted: {pred_names[i]:<15} | Actual: {true_names[i]}\")\n",
    "\n",
    "# 6. Final Accuracy\n",
    "accuracy = (correct_count / total_count) * 100\n",
    "print(f\"\\nFinal Test Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "# 7. Plot Graphs\n",
    "plot_results(loss_hist, train_acc_hist, test_acc_hist)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
