{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fdafdd8",
   "metadata": {},
   "source": [
    "# Assignemnt 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6de2465",
   "metadata": {},
   "source": [
    "# Loading the dataset, normalizing the imgaes pixel values, and one-hot encoding the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f68b336",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 # Library for dealing with images, the biggest and the fastest, written in cpp, wrapped in python.\n",
    "import os # Library for dealing wiht the operating system, getting folders, files, and much much more.\n",
    "import numpy as org_np # Numerical Python, library for doing most of the mathematical operations needed in machine learning.\n",
    "import cupy as np # NumPy, but runs on the GPU.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "def one_hot_encoding(img_true_classes, num_of_classes):\n",
    "    num_of_images = img_true_classes.shape[0]\n",
    "    one_hot_encoded_matrix = np.zeros((num_of_images, num_of_classes))\n",
    "    one_hot_encoded_matrix[np.arange(num_of_images), img_true_classes] = 1\n",
    "\n",
    "    return one_hot_encoded_matrix\n",
    "\n",
    "def load_dataset(data_dir, img_size = 64):\n",
    "    features = []\n",
    "    labels = []\n",
    "\n",
    "    classes_names = [entry.name for entry in os.scandir(data_dir) if entry.is_dir()]\n",
    "    classes_names = sorted(classes_names)\n",
    "\n",
    "    for class_index, class_name in enumerate(classes_names):\n",
    "        class_path = os.path.join(data_dir,class_name)\n",
    "        print(f\"Loading Class {class_index}: {class_name}\")\n",
    "\n",
    "        for img_name in os.listdir(class_path):\n",
    "            img_path = os.path.join(class_path, img_name)\n",
    "            \n",
    "            try:\n",
    "                img_np_arr = cv2.imread(img_path)\n",
    "\n",
    "                if img_np_arr is None:\n",
    "                    continue\n",
    "                \n",
    "                img_resized_arr = cv2.resize(img_np_arr, (img_size,img_size))\n",
    "                img_flatten_arr = img_resized_arr.flatten()\n",
    "                features.append(img_flatten_arr)\n",
    "                labels.append(class_index)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {img_name}: {e}\")\n",
    "\n",
    "\n",
    "    X = np.array(features)\n",
    "    y = np.array(labels)\n",
    "\n",
    "    X = X/255\n",
    "    y = one_hot_encoding(y, len(classes_names))\n",
    "\n",
    "    return X, y, classes_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8128a7f2",
   "metadata": {},
   "source": [
    "# Data Spliting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3fef805a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(X, y, train_ratio = 0.7, v_ratio = 0.15, test_ratio = 0.15):\n",
    "    num_of_imgs = X.shape[0]\n",
    "    indices = np.random.permutation(num_of_imgs)\n",
    "    train_index = int(train_ratio * num_of_imgs)\n",
    "    v_index = int(v_ratio * num_of_imgs)\n",
    "    test_index = int(test_ratio * num_of_imgs) # just for indication, not really needed\n",
    "\n",
    "    train_idx = indices[:train_index]\n",
    "    v_idx = indices[train_index: train_index + v_index]\n",
    "    test_idx = indices[train_index + v_index:]\n",
    "\n",
    "    X_train, X_validation, X_test = X[train_idx], X[v_idx], X[test_idx]\n",
    "    y_train, y_validation, y_test = y[train_idx], y[v_idx], y[test_idx]\n",
    "\n",
    "    return X_train, X_validation, X_test, y_train, y_validation, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569fda01",
   "metadata": {},
   "source": [
    "# Choosing the number of hidden layers, and the activation function for the hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "81516849",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_nueral_network(in_size, out_size):\n",
    "    hid_layers_num = int(input(\"So, how many hidden layers do you need for your MLP ?\\nNumber of hidden layers: \"))\n",
    "    hid_sizes = []\n",
    "\n",
    "    for i in range(hid_layers_num):\n",
    "        hid_layer_size = int(input(f\"Please enter the number of nuerons that you need in layer number {i + 1}: \"))\n",
    "        hid_sizes.append(hid_layer_size)\n",
    "\n",
    "    activ_func = int(input(\"\"\"\n",
    "                           Lastly, what is the Activation function you want to use in the hidden layers?\n",
    "                           1) Sigmoid\n",
    "                           2) Relu\n",
    "                           Choose (1/2): \n",
    "                           \"\"\"))\n",
    "\n",
    "    net_sizes = in_size + hid_sizes + out_size\n",
    "    return net_sizes , activ_func"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414b6903",
   "metadata": {},
   "source": [
    "# Intializing the first weights and biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "56725a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intialize_param(net_sizes, activ_func):        \n",
    "    weights = []\n",
    "    biases = []\n",
    "\n",
    "    for i in range(len(net_sizes) - 1):\n",
    "        input_dim = net_sizes[i] \n",
    "        output_dim =  net_sizes[i + 1]\n",
    "\n",
    "        if i < len(net_sizes) - 2:\n",
    "\n",
    "            if activ_func == 1:\n",
    "                weight = np.random.randn(output_dim, input_dim) * np.sqrt(1 / input_dim)\n",
    "\n",
    "            elif activ_func == 2:\n",
    "                weight = np.random.randn(output_dim, input_dim) * np.sqrt(2 / input_dim)\n",
    "                \n",
    "        else:\n",
    "            weight = np.random.randn(output_dim, input_dim) * np.sqrt(1 / input_dim)\n",
    "             \n",
    "        weights.append(weight)\n",
    "        bias = np.zeros((output_dim, 1))\n",
    "        biases.append(bias)\n",
    "\n",
    "    return weights, biases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d959610",
   "metadata": {},
   "source": [
    "# Activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "07fbd3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def Relu(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def SoftMax(z):\n",
    "    # 1. Numerical Stability Trick\n",
    "    # If Z contains large numbers wil cause an overflow error.\n",
    "    # By subtracting the max value from every column, the largest number becomes 0.\n",
    "    # e^0 = 1, which is safe. The math result remains identical.\n",
    "    # axis=0 means \"find max down the column\" (for each image separately).\n",
    "    # keepdims = true => keeps the values that we divide the matrix by in a proper shape to do so (as in vecotr not elements)\n",
    "\n",
    "    # SoftMax = e^z/sum(e^z)\n",
    "    shift_z = z - np.max(z, axis=0, keepdims=True)\n",
    "    exp_z = np.exp(shift_z)\n",
    "    sum_exp_z = np.sum(exp_z, axis=0, keepdims= True)\n",
    "\n",
    "    A = exp_z / sum_exp_z\n",
    "\n",
    "    return A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790a3b33",
   "metadata": {},
   "source": [
    "# Forward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4ba0cae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, weights, biases, activ_func):\n",
    "\n",
    "    A = X.T # input for each layer initialized with the input layer\n",
    "\n",
    "    # to save the values of A(inputs to the layer) and Z(Netj) for the backward propagation\n",
    "    # Why save Z? simply because we use its derivative in the backword path\n",
    "    cache_A = []\n",
    "    cache_Z = []\n",
    "    cache_A.append(A)\n",
    "\n",
    "    # enumerate gives automatic counter inside loop without the need to manually increment\n",
    "    # it returns [index, tubel(weights, biases)]\n",
    "    for layer_index, (w,b) in enumerate(zip(weights, biases)):\n",
    "        Z = np.dot(w,A) + b # Netj = Z = wx + b\n",
    "        cache_Z.append(Z)\n",
    "\n",
    "        if layer_index == len(weights) - 1: # if it is the ouput layer\n",
    "            A = SoftMax(Z)\n",
    "            cache_A.append(A)\n",
    "        else:\n",
    "            if activ_func == 1: # hidden with Sigmoid as an activation function\n",
    "                A = Sigmoid(Z)\n",
    "                cache_A.append(A)\n",
    "            elif activ_func == 2: # hidden with Relu as an acitivation function\n",
    "                A = Relu(Z)\n",
    "                cache_A.append(A)\n",
    "    \n",
    "    return cache_A, cache_Z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d5fa25",
   "metadata": {},
   "source": [
    "# Backward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b4865784",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_derivative(z):\n",
    "    sig = Sigmoid(z)\n",
    "    return sig * (1 - sig)\n",
    "\n",
    "def relu_derivative(z):\n",
    "    return (z > 0).astype(float)\n",
    "\n",
    "def backward_propagation(cache_A, cache_Z, weights, y, activ_func, m, lambda_reg = 0.0):\n",
    "    num_layers = len(weights)\n",
    "    weight_gradients = []\n",
    "    bias_gradients = []\n",
    "\n",
    "    # dA = Predictions - True Labels\n",
    "    dA = cache_A[-1] - y.T\n",
    "\n",
    "    # Backpropagate through each layer (from last to first)\n",
    "    for layer_index in range(num_layers - 1, -1, -1):\n",
    "        # Get Z and A for this layer\n",
    "        Z_curr = cache_Z[layer_index]\n",
    "        A_prev = cache_A[layer_index]\n",
    "        W_curr = weights[layer_index]\n",
    "\n",
    "        # Calculate dZ (derivative with respect to Z)\n",
    "        if layer_index == num_layers - 1:\n",
    "            dZ = dA\n",
    "        else:\n",
    "            # Hidden layers: dZ = dA * activation_derivative(Z)\n",
    "            if activ_func == 1:  # Sigmoid\n",
    "                dZ = dA * sigmoid_derivative(Z_curr)\n",
    "            elif activ_func == 2:  # ReLU\n",
    "                dZ = dA * relu_derivative(Z_curr)\n",
    "        \n",
    "        # Calculate dW (gradient for weights)\n",
    "        # dW = (1/m) * dZ * A_prev.T\n",
    "        dW = (1/m) * np.dot(dZ, A_prev.T)\n",
    "        if lambda_reg > 0.0:\n",
    "            dW += (lambda_reg / m) * W_curr\n",
    "        weight_gradients.insert(0, dW)\n",
    "        \n",
    "        # Calculate db (gradient for biases)\n",
    "        # db = (1/m) * sum of dZ along all samples\n",
    "        db = (1/m) * np.sum(dZ, axis=1, keepdims=True)\n",
    "        bias_gradients.insert(0, db)\n",
    "        \n",
    "        # Calculate dA for next iteration (previous layer)\n",
    "        # dA_prev = W.T * dZ\n",
    "        if layer_index > 0:\n",
    "            dA = np.dot(W_curr.T, dZ)\n",
    "    \n",
    "    return weight_gradients, bias_gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c771bc62",
   "metadata": {},
   "source": [
    "# Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f17c58d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(predictions, y_true, m, weights = None, lambda_reg = 0.0):\n",
    "    # Add small epsilon to avoid log(0)\n",
    "    epsilon = 1e-15\n",
    "    predictions = np.clip(predictions, epsilon, 1 - epsilon)\n",
    "\n",
    "    # Cross-entropy for multi-class classification\n",
    "    # y_true.T is (num_classes, m), predictions is (num_classes, m)\n",
    "    # We multiply element-wise and sum\n",
    "    cross_entropy_loss = -(1/m) * np.sum(y_true.T * np.log(predictions))\n",
    "    l2_reg = 0.0\n",
    "    if weights is not None and lambda_reg > 0.0:\n",
    "        for w in weights:\n",
    "            l2_reg += np.sum(w**2)\n",
    "        l2_reg = (lambda_reg / (2 * m)) * l2_reg\n",
    "\n",
    "    \n",
    "    return cross_entropy_loss + l2_reg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bb5e94",
   "metadata": {},
   "source": [
    "# Update Parameters (Gradient Descent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9fcd7de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(weights, biases, weight_gradients, bias_gradients, learning_rate):\n",
    "    updated_weights = []\n",
    "    updated_biases = []\n",
    "    \n",
    "    for i in range(len(weights)):\n",
    "        # Update weights: W = W - learning_rate * dW\n",
    "        updated_W = weights[i] - learning_rate * weight_gradients[i]\n",
    "        updated_weights.append(updated_W)\n",
    "        \n",
    "        # Update biases: b = b - learning_rate * db\n",
    "        updated_b = biases[i] - learning_rate * bias_gradients[i]\n",
    "        updated_biases.append(updated_b)\n",
    "    \n",
    "    return updated_weights, updated_biases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798b4460",
   "metadata": {},
   "source": [
    "# Actual Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8d23ea08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_acc(X, y_true_one_hot, weights, biases, activ_func):\n",
    "    a_cache, _ = forward_propagation(X, weights, biases, activ_func)\n",
    "    probabilities = a_cache[-1]\n",
    "\n",
    "    predictions = np.argmax(probabilities, axis = 0)\n",
    "    true_labels = np.argmax(y_true_one_hot, axis = 1)\n",
    "\n",
    "    acc = np.mean(predictions == true_labels)\n",
    "    return acc * 100\n",
    "\n",
    "\n",
    "def train(X_train, y_train, X_test, y_test, net_sizes, activ_func, epochs = 1000, learning_rate = 0.05, lambda_reg = 0.01):\n",
    "\n",
    "    num_of_training_imgs = X_train.shape[0]\n",
    "    weights, biases = intialize_param(net_sizes, activ_func)\n",
    "\n",
    "    loss_history = []\n",
    "    train_acc_history = []\n",
    "    test_acc_history = []\n",
    "\n",
    "    print(f\"Training on GPU for {epochs} epochs...\")\n",
    "\n",
    "    for i in range(epochs):\n",
    "        a_cache, z_cache = forward_propagation(X_train,weights,biases,activ_func)\n",
    "        weight_grads, bias_grads = backward_propagation(a_cache, z_cache, weights, y_train, activ_func, num_of_training_imgs, lambda_reg = lambda_reg)       \n",
    "        updated_weights, updated_biases = update_parameters(weights, biases, weight_grads, bias_grads, learning_rate)\n",
    "        weights, biases = updated_weights, updated_biases\n",
    "\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            current_loss = calculate_loss(a_cache[-1], y_train, num_of_training_imgs, weights = weights, lambda_reg = lambda_reg)\n",
    "            train_acc = calc_acc(X_train, y_train, weights, biases, activ_func)\n",
    "            test_acc = calc_acc(X_test, y_test, weights, biases, activ_func)\n",
    "\n",
    "            loss_history.append(current_loss)\n",
    "            train_acc_history.append(train_acc)\n",
    "            test_acc_history.append(test_acc)\n",
    "            print(f\"Epoch {i}: Loss {current_loss: .4f} | Train Acc: {train_acc: .1f}% | Test Acc: {test_acc: .1f}%\")\n",
    "\n",
    "    return weights, biases, loss_history, train_acc_history, test_acc_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0c4030",
   "metadata": {},
   "source": [
    "# Prediction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9d3a0080",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X_test, y_test, weights, biases,activ_func, classes_names):\n",
    "    a_cache, _ = forward_propagation(X_test, weights, biases, activ_func)\n",
    "    probabilities_of_output_class = a_cache[-1]\n",
    "    predicted_indices_gpu = np.argmax(probabilities_of_output_class, axis = 0)\n",
    "    true_indices_gpu = np.argmax(y_test, axis = 1)\n",
    "    predicted_indices_cpu = predicted_indices_gpu.get()\n",
    "    true_indices_cpu = true_indices_gpu.get()\n",
    "\n",
    "    predicted_names = org_np.array(classes_names)[predicted_indices_cpu]\n",
    "    true_names = org_np.array(classes_names)[true_indices_cpu]\n",
    "\n",
    "    return predicted_names, true_names\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9104399f",
   "metadata": {},
   "source": [
    "# Function for plotting some of the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "671e520f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(loss_history, train_acc, test_acc):\n",
    "    # Plot Loss and Accuracy side by side\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Subplot 1: Loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(loss_history, label='Training Loss', color='red')\n",
    "    plt.title('Loss over Epochs')\n",
    "    plt.xlabel('Epochs (x100)')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Subplot 2: Accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_acc, label='Train Accuracy', color='blue')\n",
    "    plt.plot(test_acc, label='Test Accuracy', color='green')\n",
    "    plt.title('Accuracy over Epochs')\n",
    "    plt.xlabel('Epochs (x100)')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6c8971",
   "metadata": {},
   "source": [
    "# Running the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0dcfe40a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Class 0: Clams\n",
      "Loading Class 1: Corals\n",
      "Loading Class 2: Crabs\n",
      "Loading Class 3: Dolphin\n",
      "Loading Class 4: Eel\n",
      "Loading Class 5: Fish\n",
      "Loading Class 6: Jelly_Fish\n",
      "Loading Class 7: Lobster\n",
      "Loading Class 8: Nudibranchs\n",
      "Loading Class 9: Octopus\n",
      "Loading Class 10: Otter\n",
      "Loading Class 11: Penguin\n",
      "Loading Class 12: Puffers\n",
      "Loading Class 13: Sea_Rays\n",
      "Loading Class 14: Sea_Urchins\n",
      "Loading Class 15: Seahorse\n",
      "Loading Class 16: Seal\n",
      "Loading Class 17: Sharks\n",
      "Loading Class 18: Shrimp\n",
      "Loading Class 19: Squid\n",
      "Loading Class 20: Starfish\n",
      "Loading Class 21: Turtle_Tortoise\n",
      "Loading Class 22: Whale\n",
      "\n",
      "Starting Training on GPU...\n",
      "Training on GPU for 1500 epochs...\n",
      "Epoch 0: Loss  3.2319 | Train Acc:  13.7% | Test Acc:  14.6%\n",
      "Epoch 100: Loss  2.5271 | Train Acc:  25.6% | Test Acc:  25.6%\n",
      "Epoch 200: Loss  2.3968 | Train Acc:  29.6% | Test Acc:  29.3%\n",
      "Epoch 300: Loss  2.3224 | Train Acc:  29.8% | Test Acc:  27.6%\n",
      "Epoch 400: Loss  2.3126 | Train Acc:  28.9% | Test Acc:  24.7%\n",
      "Epoch 500: Loss  2.2450 | Train Acc:  31.6% | Test Acc:  28.4%\n",
      "Epoch 600: Loss  2.1494 | Train Acc:  34.1% | Test Acc:  30.0%\n",
      "Epoch 700: Loss  2.1482 | Train Acc:  35.4% | Test Acc:  27.1%\n",
      "Epoch 800: Loss  2.1402 | Train Acc:  34.5% | Test Acc:  29.5%\n",
      "Epoch 900: Loss  2.1264 | Train Acc:  37.7% | Test Acc:  28.3%\n",
      "Epoch 1000: Loss  2.0282 | Train Acc:  40.1% | Test Acc:  30.1%\n",
      "Epoch 1100: Loss  1.9967 | Train Acc:  39.9% | Test Acc:  29.5%\n",
      "Epoch 1200: Loss  1.9730 | Train Acc:  42.6% | Test Acc:  29.6%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m net_sizes, activ_func = build_nueral_network(input_layer_size, output_layer_size)\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mStarting Training on GPU...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m trained_weights, trained_biases, loss_hist, train_acc_hist, test_acc_hist = \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnet_sizes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactiv_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mRunning Prediction on Test Set...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     13\u001b[39m pred_names, true_names = predict(X_test, y_test, trained_weights, trained_biases, activ_func, classes_names)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 26\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(X_train, y_train, X_test, y_test, net_sizes, activ_func, epochs, learning_rate, lambda_reg)\u001b[39m\n\u001b[32m     24\u001b[39m a_cache, z_cache = forward_propagation(X_train,weights,biases,activ_func)\n\u001b[32m     25\u001b[39m weight_grads, bias_grads = backward_propagation(a_cache, z_cache, weights, y_train, activ_func, num_of_training_imgs, lambda_reg = lambda_reg)       \n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m updated_weights, updated_biases = \u001b[43mupdate_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbiases\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_grads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias_grads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m weights, biases = updated_weights, updated_biases\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m i % \u001b[32m100\u001b[39m == \u001b[32m0\u001b[39m:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mupdate_parameters\u001b[39m\u001b[34m(weights, biases, weight_gradients, bias_gradients, learning_rate)\u001b[39m\n\u001b[32m      8\u001b[39m     updated_weights.append(updated_W)\n\u001b[32m     10\u001b[39m     \u001b[38;5;66;03m# Update biases: b = b - learning_rate * db\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     updated_b = biases[i] - \u001b[43mlearning_rate\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias_gradients\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     12\u001b[39m     updated_biases.append(updated_b)\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m updated_weights, updated_biases\n",
      "\u001b[36mFile \u001b[39m\u001b[32mcupy\\\\_core\\\\core.pyx:1387\u001b[39m, in \u001b[36mcupy._core.core._ndarray_base.__mul__\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mcupy\\\\_core\\\\_kernel.pyx:1333\u001b[39m, in \u001b[36mcupy._core._kernel.ufunc.__call__\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mcupy\\\\_core\\\\_kernel.pyx:1593\u001b[39m, in \u001b[36mcupy._core._kernel._Ops.guess_routine\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mcupy\\\\_core\\\\_kernel.pyx:1116\u001b[39m, in \u001b[36mcupy._core._kernel._min_scalar_type\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\numpy\\_core\\multiarray.py:642\u001b[39m, in \u001b[36mmin_scalar_type\u001b[39m\u001b[34m(a)\u001b[39m\n\u001b[32m    582\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    583\u001b[39m \u001b[33;03m    can_cast(from_, to, casting='safe')\u001b[39;00m\n\u001b[32m    584\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    637\u001b[39m \n\u001b[32m    638\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (from_,)\n\u001b[32m--> \u001b[39m\u001b[32m642\u001b[39m \u001b[38;5;129m@array_function_from_c_func_and_dispatcher\u001b[39m(_multiarray_umath.min_scalar_type)\n\u001b[32m    643\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmin_scalar_type\u001b[39m(a):\n\u001b[32m    644\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    645\u001b[39m \u001b[33;03m    min_scalar_type(a, /)\u001b[39;00m\n\u001b[32m    646\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    685\u001b[39m \n\u001b[32m    686\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m    687\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (a,)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "X, y, classes_names = load_dataset(r\"C:\\\\Life\\\\FCAI_Stuff\\\\Third_year_AI\\\\Intro_to_ML\\Assignments\\Sea_Animals\")\n",
    "X_train, X_v, X_test, y_train, y_v, y_test = split_data(X, y, train_ratio = 0.7, v_ratio = 0.15, test_ratio = 0.15)\n",
    "\n",
    "input_layer_size = [X_train.shape[1]] # 1) Number of features per image\n",
    "output_layer_size = [len(classes_names)] # 2) Number of classes\n",
    "\n",
    "net_sizes, activ_func = build_nueral_network(input_layer_size, output_layer_size)\n",
    "\n",
    "print(\"\\nStarting Training on GPU...\")\n",
    "trained_weights, trained_biases, loss_hist, train_acc_hist, test_acc_hist = train(X_train, y_train, X_test, y_test, net_sizes, activ_func, epochs = 1500, learning_rate = 0.1)\n",
    "\n",
    "print(\"\\nRunning Prediction on Test Set...\")\n",
    "pred_names, true_names = predict(X_test, y_test, trained_weights, trained_biases, activ_func, classes_names)\n",
    "\n",
    "print(\"\\n--- Test Results ---\")\n",
    "correct_count = 0\n",
    "total_count = len(pred_names)\n",
    "\n",
    "for i in range(total_count):\n",
    "    if pred_names[i] == true_names[i]:\n",
    "        correct_count += 1\n",
    "    # Simple print format\n",
    "    print(f\"Predicted: {pred_names[i]:<15} | Actual: {true_names[i]}\")\n",
    "\n",
    "# 6. Final Accuracy\n",
    "accuracy = (correct_count / total_count) * 100\n",
    "print(f\"\\nFinal Test Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "# 7. Plot Graphs\n",
    "# plot_results(loss_hist, train_acc_hist, test_acc_hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8499df14",
   "metadata": {},
   "source": [
    "# Sanity check on sklearn digits dataset (small 8x8 grayscale images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "da44f46c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Training on GPU with sklearn digits...\n",
      "Training on GPU for 800 epochs...\n",
      "Epoch 0: Loss  2.3714 | Train Acc:  9.9% | Test Acc:  12.5%\n",
      "Epoch 100: Loss  0.7355 | Train Acc:  85.5% | Test Acc:  84.5%\n",
      "Epoch 200: Loss  0.3286 | Train Acc:  91.8% | Test Acc:  91.5%\n",
      "Epoch 300: Loss  0.2027 | Train Acc:  94.9% | Test Acc:  96.3%\n",
      "Epoch 400: Loss  0.1413 | Train Acc:  96.9% | Test Acc:  97.4%\n",
      "Epoch 500: Loss  0.1074 | Train Acc:  97.9% | Test Acc:  98.5%\n",
      "Epoch 600: Loss  0.0859 | Train Acc:  98.5% | Test Acc:  98.9%\n",
      "Epoch 700: Loss  0.0707 | Train Acc:  98.8% | Test Acc:  98.9%\n",
      "\n",
      "Running Prediction on Digits Test Set...\n",
      "\n",
      "--- Digits Test Results (first 25) ---\n",
      "Predicted: 7   | Actual: 7\n",
      "Predicted: 4   | Actual: 4\n",
      "Predicted: 7   | Actual: 7\n",
      "Predicted: 9   | Actual: 9\n",
      "Predicted: 9   | Actual: 9\n",
      "Predicted: 2   | Actual: 2\n",
      "Predicted: 5   | Actual: 5\n",
      "Predicted: 5   | Actual: 5\n",
      "Predicted: 3   | Actual: 3\n",
      "Predicted: 4   | Actual: 4\n",
      "Predicted: 1   | Actual: 1\n",
      "Predicted: 7   | Actual: 7\n",
      "Predicted: 3   | Actual: 3\n",
      "Predicted: 4   | Actual: 4\n",
      "Predicted: 7   | Actual: 7\n",
      "Predicted: 6   | Actual: 6\n",
      "Predicted: 4   | Actual: 4\n",
      "Predicted: 5   | Actual: 5\n",
      "Predicted: 5   | Actual: 5\n",
      "Predicted: 0   | Actual: 0\n",
      "Predicted: 6   | Actual: 6\n",
      "Predicted: 3   | Actual: 3\n",
      "Predicted: 4   | Actual: 4\n",
      "Predicted: 1   | Actual: 1\n",
      "Predicted: 2   | Actual: 2\n",
      "\n",
      "Final Digits Test Accuracy: 98.89%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "\n",
    "def load_sklearn_digits_dataset():\n",
    "    digits = load_digits()\n",
    "    # Digits images are 8x8 with values in [0, 16]; normalize to [0, 1]\n",
    "    X_np = digits.images.astype(org_np.float32) / 16.0\n",
    "    X_flat = X_np.reshape(len(X_np), -1)\n",
    "    # Move data to GPU (cupy)\n",
    "    X = np.asarray(X_flat)\n",
    "    y_int = np.asarray(digits.target, dtype=np.int64)\n",
    "    y = one_hot_encoding(y_int, 10)\n",
    "    class_names = [str(i) for i in range(10)]\n",
    "    return X, y, class_names\n",
    "\n",
    "\n",
    "# Load digits dataset and split\n",
    "X_digits, y_digits, class_names_digits = load_sklearn_digits_dataset()\n",
    "X_train_d, X_v_d, X_test_d, y_train_d, y_v_d, y_test_d = split_data(\n",
    "    X_digits, y_digits, train_ratio=0.7, v_ratio=0.15, test_ratio=0.15\n",
    ")\n",
    "\n",
    "# Build network with same interactive helper\n",
    "input_layer_size_d = [X_train_d.shape[1]]\n",
    "output_layer_size_d = [len(class_names_digits)]\n",
    "net_sizes_d, activ_func_d = build_nueral_network(input_layer_size_d, output_layer_size_d)\n",
    "\n",
    "print(\"\\nStarting Training on GPU with sklearn digits...\")\n",
    "trained_w_d, trained_b_d, loss_hist_d, train_acc_hist_d, test_acc_hist_d = train(\n",
    "    X_train_d,\n",
    "    y_train_d,\n",
    "    X_test_d,\n",
    "    y_test_d,\n",
    "    net_sizes_d,\n",
    "    activ_func_d,\n",
    "    epochs=800,\n",
    "    learning_rate=0.1,\n",
    "    lambda_reg=0.0,\n",
    ")\n",
    "\n",
    "print(\"\\nRunning Prediction on Digits Test Set...\")\n",
    "pred_names_d, true_names_d = predict(\n",
    "    X_test_d, y_test_d, trained_w_d, trained_b_d, activ_func_d, class_names_digits\n",
    ")\n",
    "\n",
    "print(\"\\n--- Digits Test Results (first 25) ---\")\n",
    "preview_count = min(len(pred_names_d), 25)\n",
    "for i in range(preview_count):\n",
    "    print(f\"Predicted: {pred_names_d[i]:<3} | Actual: {true_names_d[i]}\")\n",
    "\n",
    "digits_accuracy = (org_np.sum(pred_names_d == true_names_d) / len(pred_names_d)) * 100\n",
    "print(f\"\\nFinal Digits Test Accuracy: {digits_accuracy:.2f}%\")\n",
    "\n",
    "# plot_results(loss_hist_d, train_acc_hist_d, test_acc_hist_d)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7604b3",
   "metadata": {},
   "source": [
    "# Harder sanity check on sklearn Olivetti faces dataset (64x64 grayscale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "242c8b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Training on GPU with Olivetti faces (harder task)...\n",
      "Training on GPU for 1200 epochs...\n",
      "Epoch 0: Loss  3.7806 | Train Acc:  4.6% | Test Acc:  0.0%\n",
      "Epoch 100: Loss  2.0340 | Train Acc:  60.0% | Test Acc:  33.3%\n",
      "Epoch 200: Loss  0.7556 | Train Acc:  92.1% | Test Acc:  76.7%\n",
      "Epoch 300: Loss  0.3011 | Train Acc:  100.0% | Test Acc:  91.7%\n",
      "Epoch 400: Loss  0.1693 | Train Acc:  100.0% | Test Acc:  95.0%\n",
      "Epoch 500: Loss  0.1113 | Train Acc:  100.0% | Test Acc:  98.3%\n",
      "Epoch 600: Loss  0.0809 | Train Acc:  100.0% | Test Acc:  98.3%\n",
      "Epoch 700: Loss  0.0631 | Train Acc:  100.0% | Test Acc:  98.3%\n",
      "Epoch 800: Loss  0.0516 | Train Acc:  100.0% | Test Acc:  98.3%\n",
      "Epoch 900: Loss  0.0438 | Train Acc:  100.0% | Test Acc:  98.3%\n",
      "Epoch 1000: Loss  0.0381 | Train Acc:  100.0% | Test Acc:  98.3%\n",
      "Epoch 1100: Loss  0.0339 | Train Acc:  100.0% | Test Acc:  98.3%\n",
      "\n",
      "Running Prediction on Olivetti Test Set...\n",
      "\n",
      "--- Olivetti Test Results (first 25) ---\n",
      "Predicted: person_34  | Actual: person_34\n",
      "Predicted: person_17  | Actual: person_17\n",
      "Predicted: person_0   | Actual: person_7\n",
      "Predicted: person_6   | Actual: person_6\n",
      "Predicted: person_8   | Actual: person_8\n",
      "Predicted: person_39  | Actual: person_39\n",
      "Predicted: person_24  | Actual: person_24\n",
      "Predicted: person_1   | Actual: person_1\n",
      "Predicted: person_16  | Actual: person_16\n",
      "Predicted: person_18  | Actual: person_18\n",
      "Predicted: person_35  | Actual: person_35\n",
      "Predicted: person_37  | Actual: person_37\n",
      "Predicted: person_30  | Actual: person_30\n",
      "Predicted: person_20  | Actual: person_20\n",
      "Predicted: person_30  | Actual: person_30\n",
      "Predicted: person_17  | Actual: person_17\n",
      "Predicted: person_33  | Actual: person_33\n",
      "Predicted: person_4   | Actual: person_4\n",
      "Predicted: person_8   | Actual: person_8\n",
      "Predicted: person_27  | Actual: person_27\n",
      "Predicted: person_10  | Actual: person_10\n",
      "Predicted: person_35  | Actual: person_35\n",
      "Predicted: person_7   | Actual: person_7\n",
      "Predicted: person_22  | Actual: person_22\n",
      "Predicted: person_6   | Actual: person_6\n",
      "\n",
      "Final Olivetti Test Accuracy: 98.33%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_olivetti_faces\n",
    "\n",
    "def load_olivetti_faces_dataset():\n",
    "    faces = fetch_olivetti_faces()\n",
    "    # faces.images: (400, 64, 64), values already in [0, 1]\n",
    "    X_np = faces.images.astype(org_np.float32)\n",
    "    X_flat = X_np.reshape(len(X_np), -1)\n",
    "    X = np.asarray(X_flat)  # move to GPU\n",
    "    y_int = np.asarray(faces.target, dtype=np.int64)\n",
    "    num_classes = int(y_int.max()) + 1\n",
    "    y = one_hot_encoding(y_int, num_classes)\n",
    "    class_names = [f\"person_{i}\" for i in range(num_classes)]\n",
    "    return X, y, class_names\n",
    "\n",
    "\n",
    "# Load Olivetti faces dataset and split\n",
    "X_faces, y_faces, class_names_faces = load_olivetti_faces_dataset()\n",
    "X_train_f, X_v_f, X_test_f, y_train_f, y_v_f, y_test_f = split_data(\n",
    "    X_faces, y_faces, train_ratio=0.7, v_ratio=0.15, test_ratio=0.15\n",
    ")\n",
    "\n",
    "# Build network for faces (input size 4096, 40 classes)\n",
    "input_layer_size_f = [X_train_f.shape[1]]\n",
    "output_layer_size_f = [len(class_names_faces)]\n",
    "net_sizes_f, activ_func_f = build_nueral_network(input_layer_size_f, output_layer_size_f)\n",
    "\n",
    "print(\"\\nStarting Training on GPU with Olivetti faces (harder task)...\")\n",
    "trained_w_f, trained_b_f, loss_hist_f, train_acc_hist_f, test_acc_hist_f = train(\n",
    "    X_train_f,\n",
    "    y_train_f,\n",
    "    X_test_f,\n",
    "    y_test_f,\n",
    "    net_sizes_f,\n",
    "    activ_func_f,\n",
    "    epochs=1200,\n",
    "    learning_rate=0.05,\n",
    "    lambda_reg=0.01,\n",
    ")\n",
    "\n",
    "print(\"\\nRunning Prediction on Olivetti Test Set...\")\n",
    "pred_names_f, true_names_f = predict(\n",
    "    X_test_f, y_test_f, trained_w_f, trained_b_f, activ_func_f, class_names_faces\n",
    ")\n",
    "\n",
    "print(\"\\n--- Olivetti Test Results (first 25) ---\")\n",
    "preview_count_f = min(len(pred_names_f), 25)\n",
    "for i in range(preview_count_f):\n",
    "    print(f\"Predicted: {pred_names_f[i]:<10} | Actual: {true_names_f[i]}\")\n",
    "\n",
    "faces_accuracy = (org_np.sum(pred_names_f == true_names_f) / len(pred_names_f)) * 100\n",
    "print(f\"\\nFinal Olivetti Test Accuracy: {faces_accuracy:.2f}%\")\n",
    "\n",
    "# plot_results(loss_hist_f, train_acc_hist_f, test_acc_hist_f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd92ff7",
   "metadata": {},
   "source": [
    "# Even harder sanity check on CIFAR-10 (32x32 RGB â†’ grayscale, resized to 64x64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cd5672de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Training on GPU with CIFAR-10 grayscale (harder task)...\n",
      "Training on GPU for 400 epochs...\n",
      "Epoch 0: Loss  2.4190 | Train Acc:  12.3% | Test Acc:  12.8%\n",
      "Epoch 100: Loss  2.0504 | Train Acc:  28.4% | Test Acc:  27.9%\n",
      "Epoch 200: Loss  1.9799 | Train Acc:  31.5% | Test Acc:  30.9%\n",
      "Epoch 300: Loss  1.9303 | Train Acc:  32.6% | Test Acc:  31.0%\n",
      "\n",
      "Running Prediction on CIFAR-10 Test Set...\n",
      "\n",
      "--- CIFAR-10 Test Results (first 25) ---\n",
      "Predicted: cat          | Actual: frog\n",
      "Predicted: ship         | Actual: horse\n",
      "Predicted: automobile   | Actual: automobile\n",
      "Predicted: bird         | Actual: frog\n",
      "Predicted: dog          | Actual: bird\n",
      "Predicted: deer         | Actual: cat\n",
      "Predicted: frog         | Actual: frog\n",
      "Predicted: deer         | Actual: cat\n",
      "Predicted: frog         | Actual: frog\n",
      "Predicted: automobile   | Actual: automobile\n",
      "Predicted: deer         | Actual: frog\n",
      "Predicted: cat          | Actual: cat\n",
      "Predicted: automobile   | Actual: automobile\n",
      "Predicted: cat          | Actual: ship\n",
      "Predicted: automobile   | Actual: frog\n",
      "Predicted: frog         | Actual: airplane\n",
      "Predicted: dog          | Actual: deer\n",
      "Predicted: automobile   | Actual: automobile\n",
      "Predicted: automobile   | Actual: ship\n",
      "Predicted: frog         | Actual: cat\n",
      "Predicted: frog         | Actual: frog\n",
      "Predicted: bird         | Actual: bird\n",
      "Predicted: bird         | Actual: bird\n",
      "Predicted: frog         | Actual: ship\n",
      "Predicted: frog         | Actual: frog\n",
      "\n",
      "Final CIFAR-10 Test Accuracy: 33.41%\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from tensorflow.keras.datasets import cifar10\n",
    "except ImportError:\n",
    "    raise ImportError(\"Please install TensorFlow to run the CIFAR-10 experiment: pip install tensorflow\")\n",
    "\n",
    "\n",
    "def load_cifar10_gray(resize_to=64, max_train=20000, max_test=5000):\n",
    "    # Load CIFAR-10 (50k train, 10k test), values in [0, 255]\n",
    "    (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "    # Optionally subsample to keep training light\n",
    "    x_train = x_train[:max_train]\n",
    "    y_train = y_train[:max_train]\n",
    "    x_test = x_test[:max_test]\n",
    "    y_test = y_test[:max_test]\n",
    "\n",
    "    # Convert to float and grayscale: Y' = 0.299 R + 0.587 G + 0.114 B\n",
    "    x_train = x_train.astype(org_np.float32)\n",
    "    x_test = x_test.astype(org_np.float32)\n",
    "    train_gray = 0.299 * x_train[:, :, :, 0] + 0.587 * x_train[:, :, :, 1] + 0.114 * x_train[:, :, :, 2]\n",
    "    test_gray = 0.299 * x_test[:, :, :, 0] + 0.587 * x_test[:, :, :, 1] + 0.114 * x_test[:, :, :, 2]\n",
    "\n",
    "    # Resize to 64x64 to match the original pipeline\n",
    "    train_resized = org_np.stack([cv2.resize(img, (resize_to, resize_to)) for img in train_gray])\n",
    "    test_resized = org_np.stack([cv2.resize(img, (resize_to, resize_to)) for img in test_gray])\n",
    "\n",
    "    # Normalize to [0, 1] and flatten\n",
    "    train_resized = (train_resized / 255.0).reshape(len(train_resized), -1)\n",
    "    test_resized = (test_resized / 255.0).reshape(len(test_resized), -1)\n",
    "\n",
    "    # Move to GPU (cupy)\n",
    "    X_train_gpu = np.asarray(train_resized)\n",
    "    X_test_gpu = np.asarray(test_resized)\n",
    "\n",
    "    y_train = y_train.flatten().astype(org_np.int64)\n",
    "    y_test = y_test.flatten().astype(org_np.int64)\n",
    "    num_classes = 10\n",
    "    y_train_oh = one_hot_encoding(np.asarray(y_train), num_classes)\n",
    "    y_test_oh = one_hot_encoding(np.asarray(y_test), num_classes)\n",
    "\n",
    "    class_names = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]\n",
    "    return X_train_gpu, y_train_oh, X_test_gpu, y_test_oh, class_names\n",
    "\n",
    "\n",
    "# Load CIFAR-10 grayscale and split using existing splitter (will re-split train portion)\n",
    "X_train_c_raw, y_train_c_raw, X_test_c_raw, y_test_c_raw, class_names_cifar = load_cifar10_gray()\n",
    "# Merge train/test then re-split to keep code consistent\n",
    "X_all_c = np.concatenate([X_train_c_raw, X_test_c_raw], axis=0)\n",
    "y_all_c = np.concatenate([y_train_c_raw, y_test_c_raw], axis=0)\n",
    "X_train_c, X_v_c, X_test_c, y_train_c, y_v_c, y_test_c = split_data(\n",
    "    X_all_c, y_all_c, train_ratio=0.7, v_ratio=0.15, test_ratio=0.15\n",
    ")\n",
    "\n",
    "# Build network for CIFAR-10 grayscale (input size 4096, 10 classes)\n",
    "input_layer_size_c = [X_train_c.shape[1]]\n",
    "output_layer_size_c = [len(class_names_cifar)]\n",
    "net_sizes_c, activ_func_c = build_nueral_network(input_layer_size_c, output_layer_size_c)\n",
    "\n",
    "print(\"\\nStarting Training on GPU with CIFAR-10 grayscale (harder task)...\")\n",
    "trained_w_c, trained_b_c, loss_hist_c, train_acc_hist_c, test_acc_hist_c = train(\n",
    "    X_train_c,\n",
    "    y_train_c,\n",
    "    X_test_c,\n",
    "    y_test_c,\n",
    "    net_sizes_c,\n",
    "    activ_func_c,\n",
    "    epochs=400,\n",
    "    learning_rate=0.05,\n",
    "    lambda_reg=0.0005,\n",
    ")\n",
    "\n",
    "print(\"\\nRunning Prediction on CIFAR-10 Test Set...\")\n",
    "pred_names_c, true_names_c = predict(\n",
    "    X_test_c, y_test_c, trained_w_c, trained_b_c, activ_func_c, class_names_cifar\n",
    ")\n",
    "\n",
    "print(\"\\n--- CIFAR-10 Test Results (first 25) ---\")\n",
    "preview_count_c = min(len(pred_names_c), 25)\n",
    "for i in range(preview_count_c):\n",
    "    print(f\"Predicted: {pred_names_c[i]:<12} | Actual: {true_names_c[i]}\")\n",
    "\n",
    "cifar_accuracy = (org_np.sum(pred_names_c == true_names_c) / len(pred_names_c)) * 100\n",
    "print(f\"\\nFinal CIFAR-10 Test Accuracy: {cifar_accuracy:.2f}%\")\n",
    "\n",
    "# plot_results(loss_hist_c, train_acc_hist_c, test_acc_hist_c)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
